# ML-Powered Pipeline for Daily AAPL Stock Price Forecasting

Goal: Predict next-day close price for AAPL (daily regression);
Dataset: Daily OHLCV+Adj_Close from Yahoo Finance, last 5 years (~1250 rows);
Pipeline: data download â†’ preprocessing â†’ train/test split â†’ feature engineering â†’ model â†’ deployment;
Moetrics: MAE/RMSE on latest predictions;

Models trained and tested: 
Sarima (classical TSA);
RandomRorest; 
XGBoost;
LSTM (Deep Learning approach, using Tensorflow/keras, Pytorch)


LSTM model with Pytorch shows the best performance. Therefore, below we describe some details of this model traning and testing. 

1.  Using Minâ€“Max scaling we scaled (normalized) the features (X) and the target (y). The scalers are fit only on the training data, then applied to validation and test â€” which prevents information leakage.

2. Sequence creation - 'def create_sequences(X, y, time_steps=60)' : 
Goal of this step - Transform each continuous 1D timeline of features into overlapping time windows (sequences).
Each sequence of time_steps = 60 days becomes one sample for the LSTM, and the label is the target value right after that window. 
Output: we have NumPy arrays (X_train_seq, y_train_seq, etc.).

But PyTorch models can only work with PyTorch tensors with GPU acceleration and automatic differentiation (autograd).

3. So, Step 4 converts all the NumPy arrays (matrix) into PyTorch tensors and prepares them for efficient mini-batch training.
X(rows, features) -> X(rows, timestep, features), i.e. 2D data (matrix) â†’ 3D sequences (tensor)	LSTM needs (batch, time, features). 
DataLoader (a PyTorch utility) breaks the dataset into mini-batches of 32 samples.

4. LSTM model:
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
        self.fc1 = nn.Linear(hidden_dim, 16)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(16, output_dim)
        
    def forward(self, x):
        out, _ = self.lstm(x)
        out = out[:, -1, :]  # take the last time step
        out = self.relu(self.fc1(out))
        out = self.fc2(out)
        return out

5. One full batch cycle of training: 
    ğŸ”¹ Step 1 â€” Forward pass:
        Input batch â†’ LSTM1 â†’ LSTM2 â†’ fc1 â†’ ReLU â†’ fc2 â†’ Output â†’ Compute prediction.

    ğŸ”¹ Step 2 â€” Compute loss:
        Loss = criterion(output, true_y). (e.g., Mean Squared Error for regression)

    ğŸ”¹ Step 3 â€” Backpropagation:
        Call loss.backward(): 
            â†’ PyTorch automatically computes gradients âˆ‚loss/âˆ‚weight for all layers.

    ğŸ”¹ Step 4 â€” Optimizer update:
        optimizer.step()
            â†’ All layer parameters are adjusted based on gradients.

    ğŸ”¹ Step 5 â€” Next batch:
        LSTM starts fresh with new sequence inputs.
            Gradients from the previous batch are cleared (optimizer.zero_grad()).
                The updated weights now slightly better fit the data â†’ model improves.



<p align="center">
  <img src="https://github.com/AKholman/stock-price-prediction-pytorch-lstm-streamlit-render/blob/main/Graph_2.png?raw=true" width="600"/>
  <br>
</p>

NN layers: 

<p align="center">
  <img src="https://github.com/AKholman/stock-price-prediction-pytorch-lstm-streamlit-render/blob/main/Graph_3.png?raw=true" width="600"/>
  <br>
</p>


<p align="center">
  <img src="https://github.com/AKholman/stock-price-prediction-pytorch-lstm-streamlit-render/blob/main/Graph.png?raw=true" width="600"/>
  <br>
</p>










Quick checklist you can copy into a project README

 Project Spec + success metrics (business & ML)
 Data pipeline: yfinance downloader + raw snapshots. GitHub
 ETL DAG (Airflow/Prefect) + data validation. Apache Airflow
 Feature store + train/test split by time.
 Experiments tracked to MLflow + model registry. MLflow
 Backtests with slippage & fees (vectorbt/backtesting.py). VectorBT +1
 Containerized serving + monitoring dashboard.
 Retraining & rollback procedure.


The project covers the following part of MLSD:

âœ… Clarify â€“ Define task (forecast next-day AAPL price).
âœ… Metrics & SLAs â€“ RMSE/MAE defined (but no SLA guarantees).
âœ… Data â€“ Yahoo Finance â†’ preprocessing pipeline.
âœ… Model choice & features â€“ Choose TSA model(s).
âœ… Training & infra â€“ Train model locally/in-app.
âœ… Serving & scaling â€“ Deploy on Streamlit + Render.
âœ… Monitoring & tradeoffs â€“ Basic (manual monitoring, dynamic re-fetch ensures live data).

âš ï¸ Not included / partially:
Automated pipelines (Airflow, Prefect).
Continuous monitoring (Prometheus, Grafana).
Advanced CI/CD + scaling infra (Kubernetes, SageMaker).
Business SLAs (availability, latency guarantees).


# ==========================
# Project: AAPL Next-Day Close Price Prediction
# Type: Daily Regression
# Full MLSD Pipeline
# ==========================

# 1. Goal Definition
# ------------------
goal = "Predict next-day close price for AAPL using daily OHLCV data"
task = Regression
# 2. Metrics & SLAs
# -----------------
metrics = ["MAE", "RMSE", "R2"]
sla = {
    "prediction_latency": "< 1 second per request",
    "accuracy_threshold": "MAE < 3 USD"
}

# 3. Data Collection
# ------------------
data_source = "Yahoo Finance"
ticker = "AAPL"
frequency = "Daily"
lookback_period = "Last 5 years (~1250 rows)"
data_pipeline = [
    "Download CSV from Yahoo Finance",
    "Load CSV into pandas DataFrame",
    "Handle missing values",
    "Feature engineering (e.g., moving averages, lag features)"
]

# 4. Train/Test Split
# ------------------
train_test_split = {
    "train": "70-80% of historical data",
    "test": "20-30% of historical data"
}

# 5. Model Choice & Training
# --------------------------
models = ["RandomForestRegressor", "XGBoostRegressor", "LinearRegression"]
training_steps = [
    "Initialize model",
    "Fit model on training data",
    "Hyperparameter tuning (GridSearchCV / RandomSearch)",
    "Validate on test set",
    "Select best-performing model"
]

# 6. Deployment / Serving
# -----------------------
deployment = {
    "framework": "Streamlit / Flask / FastAPI",
    "server": "Render / Heroku / AWS",
    "endpoint": "/predict_next_day_close"
}

# 7. Monitoring & Evaluation
# --------------------------
monitoring = [
    "Track performance metrics (MAE, RMSE) on new predictions",
    "Detect concept drift in stock prices",
    "Log predictions and input features for auditing"
]

# 8. Retraining & Feedback Loop
# -----------------------------
retraining_policy = {
    "frequency": "Weekly / Monthly",
    "trigger": "If MAE exceeds threshold or drift detected",
    "process": "Re-run training pipeline with updated historical data"
}

# 9. Tradeoffs & Considerations
# -----------------------------
tradeoffs = [
    "Latency vs accuracy",
    "Single-stock vs multi-stock scalability",
    "Feature complexity vs interpretability"
]

# End of MLSD Pipeline


**APPENDICES** 

A) LSTM MODEL BUILDING:

Input shape: (60 timesteps, 6 features)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LSTM(64, return_sequences=True) â”‚
â”‚ â†’ outputs 64 features per timestep â”‚
â”‚ Output shape: (60, 64)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
 Dropout(0.2)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LSTM(32, return_sequences=False) â”‚
â”‚ â†’ outputs only final timestep vector â”‚
â”‚ Output shape: (32,)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
 Dropout(0.2)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dense(16, activation='relu') â”‚
â”‚ â†’ fully connected layer, learns nonlinear combinations â”‚
â”‚ Output shape: (16,)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dense(1) â”‚
â”‚ â†’ final prediction (regression) â”‚
â”‚ Output shape: (1,)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


1. MODEL DEFINITION:

model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),
    Dropout(0.2),
    LSTM(32, return_sequences=False),
    Dropout(0.2),
    Dense(16, activation='relu'),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')
model.summary()

ğŸ”¹ Step 4. Data as it flows:

Step	Layer	      Input shape	     Output shape
1	    Input	       (60, 6)	   â†’     (60, 6).      usually this is not considered as a layer
2	    LSTM(64)	   (60, 6)	   â†’     (60, 64)
3	    Dropout(0.2)   (60, 64)    â†’     (60, 64)
4	    LSTM(32)	   (60, 64)	   â†’     (32,)
5	    Dropout(0.2)	(32,)	   â†’     (32,)
6	    Dense(16, ReLU)	(32,)	   â†’     (16,)
7	    Dense(1)	    (16,)	   â†’     (1,)

SUMMARY:
Total layers: 7
Input layer: (60, 6) â†’ 6 features Ã— 60 timesteps
First LSTM layer: 64 neurons (each learning a temporal pattern)
Output layer: 1 neuron (final continuous prediction)


2. STEP_BY_STEP DESCRIPTION: 

Sequential([...])

A Keras model type that stacks layers one after another. Easier for simple models (like your LSTM).
Internally, TensorFlow builds a computational graph of operations for forward and backward passes.

LSTM(64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]))
LSTM layer â†’ type of RNN for sequence data (time series, text, etc.).

64 â†’ number of hidden units (neurons in the LSTM cell). More units â†’ more capacity to learn patterns.
return_sequences=True â†’ outputs the full sequence for the next layer.
Why? Because the next LSTM layer expects a sequence as input.

input_shape=(timesteps, features) â†’ shape of one input sample:
X_train_seq.shape[1] = 60 timesteps
X_train_seq.shape[2] = 6 features per timestep

Dropout(0.2)
Regularization layer to prevent overfitting. Randomly sets 20% of the input neurons to zero during training. Helps the model not memorize training data.

LSTM(32, return_sequences=False)
Second LSTM layer stacked on the first. 32 â†’ hidden units (fewer than the first layer)
return_sequences=False â†’ outputs only the last timestep.
Why? Because the next layer is Dense, which expects a single vector, not a sequence.

Dropout(0.2)
Another dropout layer after the second LSTM. Helps prevent overfitting again.

Dense(16, activation='relu')
Fully connected (feedforward) layer. 16 â†’ number of neurons

activation='relu' â†’ non-linear activation function:
ReLU(x) = max(0, x)
Introduces non-linearity â†’ essential for neural networks to learn complex patterns

Dense(1)
Output layer. 1 neuron â†’ predicts a single value (e.g., next-day stock price). No activation â†’ linear output (common for regression)

3. MODEL COMPILATION:

model.compile(optimizer='adam', loss='mse').  optimizer='adam. Adam = Adaptive Moment Estimation. Popular gradient descent algorithm with adaptive learning rate
Handles weight updates efficiently

loss='mse'.  Mean Squared Error. Standard loss for regression tasks.
Measures difference between predicted and actual values

Keras functions involved here:
compile() â†’ prepares the model for training (sets optimizer, loss, metrics)
TensorFlow handles the math behind the scenes (forward pass, backward pass, gradient computation).

4. MODEL SUMMARY
model.summary(): 

Keras method that prints:
Each layerâ€™s name, output shape, number of parameters
Total trainable parameters (weights + biases)
Helps to verify model architecture